"""
CopilotKit ËøêË°åÊó∂ÁöÑ DeepSeek ÈÄÇÈÖçÂô®

DeepSeek ÊòØ‰∏ÄÂÆ∂‰∏ìÊ≥®‰∫é AI Á†îÁ©∂ÁöÑÂÖ¨Âè∏ÔºåÊèê‰æõÂº∫Â§ßÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊúçÂä°„ÄÇ
Êú¨ÈÄÇÈÖçÂô®‰∏∫ CopilotKit Êèê‰æõÂØπ DeepSeek AI Ê®°ÂûãÁöÑÂÆåÊï¥ÊîØÊåÅ„ÄÇ

## ‰ΩøÁî®Á§∫‰æã

```python
from copilotkit_runtime import CopilotRuntime, DeepSeekAdapter
from openai import AsyncOpenAI

# ÂàõÂª∫ËøêË°åÊó∂ÂÆû‰æã
copilot_kit = CopilotRuntime()

# ÊñπÊ≥ï 1: Áõ¥Êé•‰ΩøÁî® API Key
adapter = DeepSeekAdapter(
    api_key="your-deepseek-api-key",
    model="deepseek-chat"
)

# ÊñπÊ≥ï 2: ‰ΩøÁî®È¢ÑÈÖçÁΩÆÁöÑ OpenAI ÂÆ¢Êà∑Á´Ø
deepseek = AsyncOpenAI(
    api_key="your-deepseek-api-key",
    base_url="https://api.deepseek.com/v1",
)
adapter = DeepSeekAdapter(openai=deepseek)
```

## ÊîØÊåÅÁöÑÊ®°Âûã

DeepSeek ÊîØÊåÅ‰ª•‰∏ãÊ®°ÂûãÔºö
- deepseek-chat (ÈªòËÆ§): DeepSeek ÁöÑÊóóËà∞ÂØπËØùÊ®°ÂûãÔºåÂπ≥Ë°°ÊÄßËÉΩÂíåË¥®Èáè
- deepseek-coder: ‰∏ìÈó®ÈíàÂØπ‰ª£Á†ÅÁîüÊàêÂíåÁêÜËß£‰ºòÂåñÁöÑÊ®°Âûã
- deepseek-reasoner: Â¢ûÂº∫Êé®ÁêÜËÉΩÂäõÁöÑÊ®°ÂûãÔºåÈÄÇÂêàÂ§çÊùÇÈóÆÈ¢òËß£ÂÜ≥

## ÁâπÊÄß

- ‚úÖ ÂÆåÊï¥ÂÖºÂÆπ CopilotKit Runtime API
- ‚úÖ ÊîØÊåÅÊµÅÂºèÂìçÂ∫îÂíåÂÆûÊó∂ÊñáÊú¨ÁîüÊàê
- ‚úÖ ÊîØÊåÅÂπ∂Ë°åÂíå‰∏≤Ë°åÂ∑•ÂÖ∑Ë∞ÉÁî®
- ‚úÖ Ëá™Âä®Ê∏©Â∫¶ËåÉÂõ¥ÈôêÂà∂Ôºà0.1-2.0Ôºâ
- ‚úÖ ËßíËâ≤ËΩ¨Êç¢Ôºàdeveloper -> systemÔºâ
- ‚úÖ ÂÆåÂñÑÁöÑÈîôËØØÂ§ÑÁêÜÂíåÊó•ÂøóËÆ∞ÂΩï
"""

import asyncio
import json
import logging
from typing import Optional, Dict, Any, AsyncGenerator
from uuid import uuid4

from openai import AsyncOpenAI
from openai.types.chat import ChatCompletionChunk

from .base import CopilotServiceAdapter
from ..types import (
    CopilotRuntimeChatCompletionRequest,
    CopilotRuntimeChatCompletionResponse,
)
from ..utils import (
    convert_action_to_openai_tool,
    convert_message_to_openai_message,
    limit_messages_to_token_count,
)
from ..events import EventSource

logger = logging.getLogger(__name__)

DEFAULT_MODEL = "deepseek-chat"
DEEPSEEK_BASE_URL = "https://api.deepseek.com/v1"


class DeepSeekAdapter(CopilotServiceAdapter):
    """
    CopilotKit ËøêË°åÊó∂ÁöÑ DeepSeek ÈÄÇÈÖçÂô®
    
    Êèê‰æõ‰∏é DeepSeek AI Ê®°ÂûãÁöÑÈõÜÊàêÔºåÂåÖÊã¨ deepseek-chat„ÄÅ
    deepseek-coder Âíå deepseek-reasoner Ê®°Âûã„ÄÇ
    
    Ê≠§ÈÄÇÈÖçÂô®Ë¥üË¥£Ôºö
    - Â∞Ü CopilotKit ËØ∑Ê±ÇËΩ¨Êç¢‰∏∫ DeepSeek API Ë∞ÉÁî®
    - Â§ÑÁêÜÊµÅÂºèÂìçÂ∫îÂíå‰∫ã‰ª∂ÊµÅ
    - ÁÆ°ÁêÜÂ∑•ÂÖ∑Ë∞ÉÁî®ÂíåÂèÇÊï∞‰º†ÈÄí
    - Â§ÑÁêÜÈîôËØØÂíåÂºÇÂ∏∏ÊÉÖÂÜµ
    - Êèê‰æõÂÆåÊï¥ÁöÑÊó•ÂøóËÆ∞ÂΩï
    
    ÂÖºÂÆπÊÄßÔºö
    - ‰∏é TypeScript ÁâàÊú¨ÁöÑ DeepSeekAdapter ÂÆåÂÖ®ÂÖºÂÆπ
    - ÊîØÊåÅÊâÄÊúâ OpenAI ÂÖºÂÆπÁöÑÂèÇÊï∞ÂíåÈÖçÁΩÆ
    - Ëá™Âä®Â§ÑÁêÜ DeepSeek ÁâπÂÆöÁöÑÈôêÂà∂ÂíåË¶ÅÊ±Ç
    """
    
    def __init__(
        self,
        openai: Optional[AsyncOpenAI] = None,
        api_key: Optional[str] = None,
        model: str = DEFAULT_MODEL,
        disable_parallel_tool_calls: bool = False,
        base_url: str = DEEPSEEK_BASE_URL,
        headers: Optional[Dict[str, str]] = None,
    ):
        """
        ÂàùÂßãÂåñ DeepSeek ÈÄÇÈÖçÂô®
        
        ÂèÇÊï∞:
            openai: È¢ÑÈÖçÁΩÆÁöÑ AsyncOpenAI ÂÆû‰æãÔºåÁî®‰∫é DeepSeek API Ë∞ÉÁî®
            api_key: DeepSeek API ÂØÜÈí•ÔºàÂ¶ÇÊûúÊú™Êèê‰æõ openai ÂÆû‰æãÂàôÂøÖÈúÄÔºâ
            model: Ë¶Å‰ΩøÁî®ÁöÑ DeepSeek Ê®°Âûã
                   - deepseek-chat: ÈÄöÁî®ÂØπËØùÊ®°ÂûãÔºàÈªòËÆ§Ôºâ
                   - deepseek-coder: ‰ª£Á†Å‰∏ìÁî®Ê®°Âûã
                   - deepseek-reasoner: Êé®ÁêÜÂ¢ûÂº∫Ê®°Âûã
            disable_parallel_tool_calls: ÊòØÂê¶Á¶ÅÁî®Âπ∂Ë°åÂ∑•ÂÖ∑Ë∞ÉÁî®
            base_url: Ëá™ÂÆö‰πâ DeepSeek API Âü∫Á°Ä URL
            headers: ÂèëÈÄÅËØ∑Ê±ÇÊó∂ÁöÑÈ¢ùÂ§ñÂ§¥ÈÉ®‰ø°ÊÅØ
        
        ÂºÇÂ∏∏:
            ValueError: ÂΩìÊú™Êèê‰æõ openai ÂÆû‰æã‰∏îÁº∫Â∞ë api_key Êó∂ÊäõÂá∫
        
        ‰ΩøÁî®Á§∫‰æã:
            # ‰ΩøÁî® API Key ÂàùÂßãÂåñ
            adapter = DeepSeekAdapter(api_key="sk-xxx", model="deepseek-chat")
            
            # ‰ΩøÁî®È¢ÑÈÖçÁΩÆÂÆ¢Êà∑Á´ØÂàùÂßãÂåñ
            client = AsyncOpenAI(api_key="sk-xxx", base_url="https://api.deepseek.com/v1")
            adapter = DeepSeekAdapter(openai=client)
        """
        if openai:
            self._openai = openai
        else:
            if not api_key:
                raise ValueError("DeepSeek API key is required when openai instance is not provided")
            
            default_headers = {
                "User-Agent": "CopilotKit-DeepSeek-Adapter",
            }
            if headers:
                default_headers.update(headers)
                
            self._openai = AsyncOpenAI(
                api_key=api_key,
                base_url=base_url,
                default_headers=default_headers,
            )
        
        self.model = model
        self.disable_parallel_tool_calls = disable_parallel_tool_calls
        
    @property
    def openai(self) -> AsyncOpenAI:
        """Get the OpenAI client instance."""
        return self._openai
    
    async def process(
        self, request: CopilotRuntimeChatCompletionRequest
    ) -> CopilotRuntimeChatCompletionResponse:
        """Process chat completion request using DeepSeek."""
        thread_id = request.thread_id or str(uuid4())
        model = request.model or self.model
        messages = request.messages
        actions = request.actions
        event_source = request.event_source
        forwarded_parameters = request.forwarded_parameters or {}
        
        logger.info(
            "üîÑ [DeepSeek] Processing request: thread_id=%s, model=%s, messages=%d, actions=%d",
            thread_id, model, len(messages), len(actions)
        )
        
        # Convert actions to OpenAI tools
        tools = [convert_action_to_openai_tool(action) for action in actions]
        
        # Filter messages using allowlist approach
        valid_tool_use_ids = set()
        
        # Extract valid tool_call IDs
        for message in messages:
            if message.is_action_execution_message():
                valid_tool_use_ids.add(message.id)
        
        # Filter messages, keeping only those with valid tool_call IDs
        filtered_messages = []
        for message in messages:
            if message.is_result_message():
                # Skip if there's no corresponding tool_call
                if message.action_execution_id not in valid_tool_use_ids:
                    continue
                # Remove this ID from valid IDs so we don't process duplicates
                valid_tool_use_ids.discard(message.action_execution_id)
            filtered_messages.append(message)
        
        # Convert to OpenAI messages
        openai_messages = [
            convert_message_to_openai_message(m, keep_system_role=True)
            for m in filtered_messages
        ]
        
        # DeepSeek compatibility fix: convert unsupported 'developer' role to 'system'
        for message in openai_messages:
            if isinstance(message, dict) and message.get("role") == "developer":
                logger.info("üîÑ [DeepSeek] Converting developer role to system role")
                message["role"] = "system"
        
        # Limit messages to token count
        openai_messages = limit_messages_to_token_count(openai_messages, tools, model)
        
        # Prepare tool choice
        tool_choice = forwarded_parameters.get("tool_choice")
        if tool_choice == "function":
            tool_choice = {
                "type": "function",
                "function": {"name": forwarded_parameters.get("tool_choice_function_name")},
            }
        
        logger.info(
            "üì§ [DeepSeek] Sending request to API: model=%s, messages=%d, tools=%d",
            model, len(openai_messages), len(tools)
        )
        
        try:
            # Prepare request payload
            request_payload = {
                "model": model,
                "stream": True,
                "messages": openai_messages,
            }
            
            if tools:
                request_payload["tools"] = tools
            
            if forwarded_parameters.get("max_tokens"):
                request_payload["max_tokens"] = forwarded_parameters["max_tokens"]
                
            if forwarded_parameters.get("stop"):
                request_payload["stop"] = forwarded_parameters["stop"]
                
            if tool_choice:
                request_payload["tool_choice"] = tool_choice
                
            if self.disable_parallel_tool_calls:
                request_payload["parallel_tool_calls"] = False
                
            if forwarded_parameters.get("temperature"):
                # DeepSeek temperature range: 0.1-2.0
                temperature = max(0.1, min(2.0, forwarded_parameters["temperature"]))
                request_payload["temperature"] = temperature
            
            logger.debug("üì§ [DeepSeek] API Request payload: %s", json.dumps(request_payload, indent=2))
            
            # Create stream
            stream = await self._openai.chat.completions.create(**request_payload)
            
            logger.info("üîÑ [DeepSeek] Stream created successfully, starting to process...")
            
            # Process stream
            await self._process_stream(stream, event_source, thread_id)
            
            return CopilotRuntimeChatCompletionResponse(thread_id=thread_id)
            
        except Exception as error:
            logger.error("‚ùå [DeepSeek] API error: %s", error)
            raise RuntimeError(f"DeepSeek API request failed: {error}")
    
    async def _process_stream(
        self,
        stream: AsyncGenerator[ChatCompletionChunk, None],
        event_source: EventSource,
        thread_id: str,
    ) -> None:
        """Process the streaming response from DeepSeek."""
        
        async def stream_handler(event_stream):
            mode = None  # "function" | "message" | None
            current_message_id = ""
            current_tool_call_id = ""
            current_action_name = ""
            
            try:
                logger.info("üîÑ [DeepSeek] Starting stream iteration...")
                chunk_count = 0
                
                async for chunk in stream:
                    chunk_count += 1
                    
                    logger.debug(
                        "üì¶ [DeepSeek] Received chunk #%d: choices=%d, finish_reason=%s",
                        chunk_count,
                        len(chunk.choices),
                        chunk.choices[0].finish_reason if chunk.choices else None,
                    )
                    
                    if not chunk.choices:
                        continue
                    
                    choice = chunk.choices[0]
                    tool_call = choice.delta.tool_calls[0] if choice.delta.tool_calls else None
                    content = choice.delta.content
                    finish_reason = choice.finish_reason
                    
                    if finish_reason:
                        logger.info("üèÅ [DeepSeek] Finish reason detected: %s", finish_reason)
                    
                    # Mode switching logic (from OpenAI adapter)
                    if mode == "message" and tool_call and tool_call.id:
                        logger.info("üîß [DeepSeek] Switching from message to function mode")
                        mode = None
                        await event_stream.send_text_message_end(message_id=current_message_id)
                    elif mode == "function" and (not tool_call or tool_call.id):
                        logger.info("üîß [DeepSeek] Switching from function to message mode")
                        mode = None
                        await event_stream.send_action_execution_end(
                            action_execution_id=current_tool_call_id
                        )
                    
                    # Start new mode if needed
                    if mode is None:
                        if tool_call and tool_call.id:
                            logger.info("üöÄ [DeepSeek] Starting function mode")
                            mode = "function"
                            current_tool_call_id = tool_call.id
                            current_action_name = tool_call.function.name if tool_call.function else ""
                            await event_stream.send_action_execution_start(
                                action_execution_id=current_tool_call_id,
                                parent_message_id=chunk.id,
                                action_name=current_action_name,
                            )
                        elif content:
                            logger.info("üí¨ [DeepSeek] Starting message mode")
                            mode = "message"
                            current_message_id = chunk.id or str(uuid4())
                            await event_stream.send_text_message_start(message_id=current_message_id)
                    
                    # Send content events
                    if mode == "message" and content:
                        logger.debug("üí¨ [DeepSeek] Sending text content: %s", content)
                        await event_stream.send_text_message_content(
                            message_id=current_message_id,
                            content=content,
                        )
                    elif mode == "function" and tool_call and tool_call.function and tool_call.function.arguments:
                        logger.debug("üìù [DeepSeek] Sending function arguments: %s", tool_call.function.arguments)
                        await event_stream.send_action_execution_args(
                            action_execution_id=current_tool_call_id,
                            args=tool_call.function.arguments,
                        )
                    
                    # Break on finish reason
                    if finish_reason:
                        logger.info("üîö [DeepSeek] Breaking loop due to finish reason: %s", finish_reason)
                        break
                
                # Send final end events
                logger.info("üèÅ [DeepSeek] Stream loop ended after %d chunks, sending final events", chunk_count)
                
                if mode == "message":
                    logger.info("üí¨ [DeepSeek] Ending final text message")
                    await event_stream.send_text_message_end(message_id=current_message_id)
                elif mode == "function":
                    logger.info("üîß [DeepSeek] Ending final function execution")
                    await event_stream.send_action_execution_end(
                        action_execution_id=current_tool_call_id
                    )
                    
            except Exception as error:
                logger.error("‚ùå [DeepSeek] streaming error: %s", error)
                
                # Error cleanup
                if mode == "message":
                    logger.info("üí¨ [DeepSeek] Error cleanup: ending text message")
                    await event_stream.send_text_message_end(message_id=current_message_id)
                elif mode == "function" and current_tool_call_id:
                    logger.info("üîß [DeepSeek] Error cleanup: ending function execution")
                    await event_stream.send_action_execution_end(
                        action_execution_id=current_tool_call_id
                    )
                raise error
            
            # Complete event stream
            logger.info("üéâ [DeepSeek] Completing event stream")
            await event_stream.complete()
        
        await event_source.stream(stream_handler) 