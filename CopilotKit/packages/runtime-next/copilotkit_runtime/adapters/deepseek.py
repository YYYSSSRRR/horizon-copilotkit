"""
CopilotKit Python è¿è¡Œæ—¶çš„ DeepSeek é€‚é…å™¨

è¿™æ˜¯åŸºäº TypeScript ç‰ˆæœ¬å®ç°çš„å®Œæ•´ DeepSeek é€‚é…å™¨ï¼Œæ”¯æŒï¼š
- æµå¼å“åº”å¤„ç†
- å·¥å…·è°ƒç”¨å’Œå‡½æ•°æ‰§è¡Œ
- å®Œæ•´çš„äº‹ä»¶æºé›†æˆ
- DeepSeek ç‰¹å®šçš„å…¼å®¹æ€§å¤„ç†

## ä½¿ç”¨ç¤ºä¾‹

```python
import os
from copilotkit_runtime import CopilotRuntime, DeepSeekAdapter
from openai import AsyncOpenAI

# æ–¹å¼ 1: ç›´æ¥ä½¿ç”¨ API å¯†é’¥
adapter = DeepSeekAdapter(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    model="deepseek-chat"
)

# æ–¹å¼ 2: ä½¿ç”¨è‡ªå®šä¹‰ OpenAI å®¢æˆ·ç«¯
deepseek_client = AsyncOpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com/v1",
)
adapter = DeepSeekAdapter(openai=deepseek_client)

runtime = CopilotRuntime(adapter=adapter)
```

## æ”¯æŒçš„æ¨¡å‹

- deepseek-chat (é»˜è®¤): DeepSeek çš„æ——èˆ°å¯¹è¯æ¨¡å‹
- deepseek-coder: ä»£ç ä¸“ç”¨æ¨¡å‹ï¼Œä¼˜åŒ–äº†ä»£ç ç”Ÿæˆå’Œç†è§£èƒ½åŠ›
- deepseek-reasoner: æ¨ç†å¢å¼ºæ¨¡å‹ï¼Œé€‚åˆå¤æ‚é—®é¢˜è§£å†³
"""

import asyncio
import json
import logging
from typing import Any, Dict, List, Optional, Set
import time
import uuid

from openai import AsyncOpenAI
from pydantic import BaseModel, Field

from ..types.adapters import CopilotServiceAdapter, AdapterRequest, AdapterResponse
from ..types.messages import (
    Message, 
    TextMessage, 
    ActionExecutionMessage, 
    ResultMessage, 
    MessageRole,
    MessageType
)
from ..types.actions import ActionInput
from ..types.core import ForwardedParameters
from ..events.runtime_events import RuntimeEventSource, RuntimeEvent
from ..utils.common import generate_id


logger = logging.getLogger(__name__)

# å¸¸é‡å®šä¹‰
DEFAULT_MODEL = "deepseek-chat"
DEEPSEEK_BASE_URL = "https://api.deepseek.com/v1"
DEEPSEEK_TEMPERATURE_MIN = 0.1
DEEPSEEK_TEMPERATURE_MAX = 2.0


class DeepSeekConfig(BaseModel):
    """DeepSeek é€‚é…å™¨é…ç½®"""
    api_key: Optional[str] = Field(None, description="DeepSeek API å¯†é’¥")
    model: str = Field(DEFAULT_MODEL, description="æ¨¡å‹åç§°")
    base_url: str = Field(DEEPSEEK_BASE_URL, description="API åŸºç¡€URL")
    max_tokens: Optional[int] = Field(None, description="æœ€å¤§tokenæ•°")
    temperature: float = Field(0.7, description="æ¸©åº¦å‚æ•°")
    timeout: float = Field(60.0, description="è¯·æ±‚è¶…æ—¶æ—¶é—´")
    max_retries: int = Field(3, description="æœ€å¤§é‡è¯•æ¬¡æ•°")
    disable_parallel_tool_calls: bool = Field(False, description="æ˜¯å¦ç¦ç”¨å¹¶è¡Œå·¥å…·è°ƒç”¨")
    headers: Optional[Dict[str, str]] = Field(None, description="é¢å¤–è¯·æ±‚å¤´")


class EventStream:
    """æ¨¡æ‹Ÿ TypeScript ç‰ˆæœ¬çš„ eventStream$"""
    
    def __init__(self, event_source: RuntimeEventSource):
        self.event_source = event_source
        
    async def send_text_message_start(self, data: Dict[str, Any]):
        """å‘é€æ–‡æœ¬æ¶ˆæ¯å¼€å§‹äº‹ä»¶"""
        await self.event_source.emit(RuntimeEvent(
            type="text_message_start",
            data=data
        ))
    
    async def send_text_message_content(self, data: Dict[str, Any]):
        """å‘é€æ–‡æœ¬æ¶ˆæ¯å†…å®¹äº‹ä»¶"""
        await self.event_source.emit(RuntimeEvent(
            type="text_message_content", 
            data=data
        ))
    
    async def send_text_message_end(self, data: Dict[str, Any]):
        """å‘é€æ–‡æœ¬æ¶ˆæ¯ç»“æŸäº‹ä»¶"""
        await self.event_source.emit(RuntimeEvent(
            type="text_message_end",
            data=data
        ))
    
    async def send_action_execution_start(self, data: Dict[str, Any]):
        """å‘é€åŠ¨ä½œæ‰§è¡Œå¼€å§‹äº‹ä»¶"""
        await self.event_source.emit(RuntimeEvent(
            type="action_execution_start",
            data=data
        ))
    
    async def send_action_execution_args(self, data: Dict[str, Any]):
        """å‘é€åŠ¨ä½œæ‰§è¡Œå‚æ•°äº‹ä»¶"""
        await self.event_source.emit(RuntimeEvent(
            type="action_execution_args",
            data=data
        ))
    
    async def send_action_execution_end(self, data: Dict[str, Any]):
        """å‘é€åŠ¨ä½œæ‰§è¡Œç»“æŸäº‹ä»¶"""
        await self.event_source.emit(RuntimeEvent(
            type="action_execution_end",
            data=data
        ))
    
    def complete(self):
        """æ ‡è®°æµå®Œæˆ"""
        # åœ¨ Python ä¸­è¿™å¯èƒ½ä¸éœ€è¦ç‰¹æ®Šå¤„ç†
        pass


class DeepSeekAdapter(CopilotServiceAdapter):
    """
    DeepSeek API é€‚é…å™¨
    
    å®ç°äº†ä¸ DeepSeek API çš„å®Œæ•´é›†æˆï¼ŒåŒ…æ‹¬æµå¼å“åº”ã€å·¥å…·è°ƒç”¨ç­‰åŠŸèƒ½ã€‚
    """
    
    def __init__(self, openai: Optional[AsyncOpenAI] = None, **kwargs):
        """
        åˆå§‹åŒ– DeepSeek é€‚é…å™¨
        
        Args:
            openai: å¯é€‰çš„ AsyncOpenAI å®¢æˆ·ç«¯å®ä¾‹
            **kwargs: å…¶ä»–é…ç½®å‚æ•°
        """
        # å¦‚æœæä¾›äº† openai å®¢æˆ·ç«¯ï¼Œç›´æ¥ä½¿ç”¨
        if openai is not None:
            self._openai = openai
            # ä» kwargs ä¸­æå–é…ç½®ï¼Œä½† api_key ä½¿ç”¨å®¢æˆ·ç«¯çš„
            config_dict = kwargs.copy()
            if "api_key" not in config_dict:
                config_dict["api_key"] = "provided_via_client"
            self.config = DeepSeekConfig(**config_dict)
        else:
            # å¦åˆ™æ ¹æ®é…ç½®åˆ›å»ºå®¢æˆ·ç«¯
            self.config = DeepSeekConfig(**kwargs)
            if not self.config.api_key:
                raise ValueError("DeepSeek API key is required when openai instance is not provided")
            
            self._openai = AsyncOpenAI(
                api_key=self.config.api_key,
                base_url=self.config.base_url,
                timeout=self.config.timeout,
                max_retries=self.config.max_retries,
                default_headers={
                    "User-Agent": "CopilotKit-DeepSeek-Adapter",
                    **(self.config.headers or {})
                }
            )
        
        logger.info(f"ğŸš€ DeepSeekAdapter initialized with model: {self.config.model}")
    
    @property
    def openai(self) -> AsyncOpenAI:
        """è·å– OpenAI å®¢æˆ·ç«¯"""
        return self._openai
    
    def get_provider_name(self) -> str:
        """è·å–æä¾›å•†åç§°"""
        return "deepseek"
    
    def get_model_name(self) -> str:
        """è·å–æ¨¡å‹åç§°"""
        return self.config.model
    
    def get_model(self) -> Optional[str]:
        """è·å–é»˜è®¤æ¨¡å‹åç§°"""
        return self.config.model
    
    async def process(self, request: AdapterRequest) -> AdapterResponse:
        """
        å¤„ç†èŠå¤©å®Œæˆè¯·æ±‚
        
        Args:
            request: é€‚é…å™¨è¯·æ±‚
            
        Returns:
            é€‚é…å™¨å“åº”
        """
        thread_id = request.thread_id or generate_id()
        model = request.model or self.config.model
        messages = request.messages
        actions = request.actions or []
        event_source = request.event_source
        forwarded_parameters = request.forwarded_parameters or ForwardedParameters()
        
        logger.info(f"ğŸ”„ [DeepSeek] Processing request: thread_id={thread_id}, model={model}, "
                   f"messages_count={len(messages)}, actions_count={len(actions)}")
        
        # è½¬æ¢åŠ¨ä½œä¸º OpenAI å·¥å…·æ ¼å¼
        tools = [self._convert_action_to_openai_tool(action) for action in actions]
        
        # è¿‡æ»¤å’Œå¤„ç†æ¶ˆæ¯ï¼ˆå®ç° TypeScript ç‰ˆæœ¬çš„ ALLOWLIST é€»è¾‘ï¼‰
        filtered_messages = self._filter_messages_with_allowlist(messages)
        
        # è½¬æ¢æ¶ˆæ¯ä¸º OpenAI æ ¼å¼
        openai_messages = [
            self._convert_message_to_openai(msg) for msg in filtered_messages
        ]
        
        # DeepSeek å…¼å®¹æ€§ä¿®å¤ï¼šå°† 'developer' è§’è‰²è½¬æ¢ä¸º 'system'
        openai_messages = self._fix_deepseek_compatibility(openai_messages)
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_payload = self._build_request_payload(
            model=model,
            messages=openai_messages,
            tools=tools,
            forwarded_parameters=forwarded_parameters
        )
        
        logger.debug(f"ğŸ“¤ [DeepSeek] API Request payload: {json.dumps(request_payload, indent=2)}")
        
        try:
            # åˆ›å»ºæµå¼è¯·æ±‚
            stream = await self._openai.chat.completions.create(**request_payload)
            
            logger.info("ğŸ”„ [DeepSeek] Stream created successfully, starting to process...")
            
            # å¦‚æœæœ‰äº‹ä»¶æºï¼Œå¤„ç†æµå¼å“åº”
            if event_source:
                await self._process_streaming_response(
                    stream=stream,
                    event_source=event_source,
                    thread_id=thread_id,
                    actions=actions
                )
            
            return AdapterResponse(
                thread_id=thread_id,
                event_source=event_source
            )
            
        except Exception as error:
            logger.error(f"âŒ [DeepSeek] API error: {error}")
            
            # å¦‚æœæœ‰äº‹ä»¶æºï¼Œå‘é€é”™è¯¯äº‹ä»¶
            if event_source:
                await event_source.emit(RuntimeEvent(
                    type="error",
                    data={
                        "error": str(error),
                        "provider": "deepseek",
                        "threadId": thread_id
                    }
                ))
            
            raise Exception(f"DeepSeek API request failed: {error}")
    
    def _filter_messages_with_allowlist(self, messages: List[Message]) -> List[Message]:
        """
        ä½¿ç”¨ ALLOWLIST æ–¹æ³•è¿‡æ»¤æ¶ˆæ¯ï¼ˆå¤åˆ¶ TypeScript ç‰ˆæœ¬çš„é€»è¾‘ï¼‰
        
        åªåŒ…å«å¯¹åº”æœ‰æ•ˆ tool_call çš„ tool_result æ¶ˆæ¯
        """
        # æ­¥éª¤ 1: æå–æœ‰æ•ˆçš„ tool_call ID
        valid_tool_use_ids: Set[str] = set()
        
        for message in messages:
            if message.is_action_execution_message():
                valid_tool_use_ids.add(message.id)
        
        # æ­¥éª¤ 2: è¿‡æ»¤æ¶ˆæ¯ï¼Œåªä¿ç•™æœ‰æ•ˆ tool_call ID çš„æ¶ˆæ¯
        filtered_messages = []
        
        for message in messages:
            if message.is_result_message():
                # å¦‚æœæ²¡æœ‰å¯¹åº”çš„ tool_callï¼Œè·³è¿‡
                if message.action_execution_id not in valid_tool_use_ids:
                    continue
                
                # ä»æœ‰æ•ˆ ID ä¸­ç§»é™¤ï¼Œé¿å…å¤„ç†é‡å¤
                valid_tool_use_ids.discard(message.action_execution_id)
                filtered_messages.append(message)
            else:
                # ä¿ç•™æ‰€æœ‰éå·¥å…·ç»“æœæ¶ˆæ¯
                filtered_messages.append(message)
        
        return filtered_messages
    
    def _convert_message_to_openai(self, message: Message) -> Dict[str, Any]:
        """å°† CopilotKit æ¶ˆæ¯è½¬æ¢ä¸º OpenAI æ ¼å¼"""
        if isinstance(message, TextMessage):
            return {
                "role": message.role.value,
                "content": message.content
            }
        elif isinstance(message, ActionExecutionMessage):
            return {
                "role": "assistant",
                "content": None,
                "tool_calls": [{
                    "id": message.id,
                    "type": "function",
                    "function": {
                        "name": message.name,
                        "arguments": json.dumps(message.arguments)
                    }
                }]
            }
        elif isinstance(message, ResultMessage):
            return {
                "role": "tool",
                "tool_call_id": message.action_execution_id,
                "content": json.dumps(message.result) if not isinstance(message.result, str) else message.result
            }
        else:
            # é»˜è®¤å¤„ç†
            return {
                "role": "user",
                "content": str(message)
            }
    
    def _fix_deepseek_compatibility(self, openai_messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        DeepSeek å…¼å®¹æ€§ä¿®å¤ï¼šå°†ä¸æ”¯æŒçš„ 'developer' è§’è‰²è½¬æ¢ä¸º 'system'
        """
        fixed_messages = []
        
        for message in openai_messages:
            if message and isinstance(message, dict) and message.get('role') == 'developer':
                logger.debug('ğŸ”„ [DeepSeek] Converting developer role to system role')
                fixed_message = message.copy()
                fixed_message['role'] = 'system'
                fixed_messages.append(fixed_message)
            else:
                fixed_messages.append(message)
        
        return fixed_messages
    
    def _convert_action_to_openai_tool(self, action: ActionInput) -> Dict[str, Any]:
        """å°† CopilotKit åŠ¨ä½œè½¬æ¢ä¸º OpenAI å·¥å…·æ ¼å¼"""
        properties = {}
        required = []
        
        for param in action.parameters:
            properties[param.name] = {
                "type": param.type.value if hasattr(param.type, 'value') else str(param.type),
                "description": param.description
            }
            
            if param.required:
                required.append(param.name)
        
        return {
            "type": "function",
            "function": {
                "name": action.name,
                "description": action.description,
                "parameters": {
                    "type": "object",
                    "properties": properties,
                    "required": required
                }
            }
        }
    
    def _build_request_payload(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        tools: List[Dict[str, Any]],
        forwarded_parameters: ForwardedParameters
    ) -> Dict[str, Any]:
        """æ„å»º DeepSeek API è¯·æ±‚è´Ÿè½½"""
        payload = {
            "model": model,
            "stream": True,
            "messages": messages,
        }
        
        # æ·»åŠ å·¥å…·
        if tools:
            payload["tools"] = tools
            
            # å¤„ç†å·¥å…·é€‰æ‹©
            tool_choice = forwarded_parameters.tool_choice
            if tool_choice == "function" and forwarded_parameters.tool_choice_function_name:
                payload["tool_choice"] = {
                    "type": "function",
                    "function": {"name": forwarded_parameters.tool_choice_function_name}
                }
            elif tool_choice:
                payload["tool_choice"] = tool_choice
            
            # å¤„ç†å¹¶è¡Œå·¥å…·è°ƒç”¨
            if self.config.disable_parallel_tool_calls:
                payload["parallel_tool_calls"] = False
        
        # æ·»åŠ å…¶ä»–å‚æ•°
        if forwarded_parameters.max_tokens:
            payload["max_tokens"] = forwarded_parameters.max_tokens
        
        if forwarded_parameters.stop:
            payload["stop"] = forwarded_parameters.stop
        
        if forwarded_parameters.temperature is not None:
            # DeepSeek æ¸©åº¦èŒƒå›´é™åˆ¶
            temperature = max(
                DEEPSEEK_TEMPERATURE_MIN,
                min(DEEPSEEK_TEMPERATURE_MAX, forwarded_parameters.temperature)
            )
            payload["temperature"] = temperature
        
        return payload
    
    async def _process_streaming_response(
        self,
        stream,
        event_source: RuntimeEventSource,
        thread_id: str,
        actions: List[ActionInput]
    ):
        """å¤„ç†æµå¼å“åº”ï¼ˆå¤åˆ¶ TypeScript ç‰ˆæœ¬çš„å®Œæ•´é€»è¾‘ï¼‰"""
        
        async def stream_callback(event_stream_iter):
            event_stream = EventStream(event_source)
            
            mode: Optional[str] = None  # "function" | "message" | None
            current_message_id: str = ""
            current_tool_call_id: str = ""
            current_action_name: str = ""
            
            try:
                logger.info("ğŸ”„ [DeepSeek] Starting stream iteration...")
                chunk_count = 0
                
                async for chunk in stream:
                    chunk_count += 1
                    
                    logger.debug(f"ğŸ“¦ [DeepSeek] Received chunk #{chunk_count}: "
                               f"choices_length={len(chunk.choices)}, "
                               f"finish_reason={chunk.choices[0].finish_reason if chunk.choices else None}")
                    
                    if not chunk.choices:
                        continue
                    
                    choice = chunk.choices[0]
                    tool_call = choice.delta.tool_calls[0] if choice.delta.tool_calls else None
                    content = choice.delta.content
                    finish_reason = choice.finish_reason
                    
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»“æŸæµ
                    if finish_reason:
                        logger.info(f"ğŸ [DeepSeek] Finish reason detected: {finish_reason}")
                    
                    # æ¨¡å¼åˆ‡æ¢é€»è¾‘ï¼ˆæ¥è‡ª TypeScript ç‰ˆæœ¬ï¼‰
                    # ä»æ¶ˆæ¯æ¨¡å¼åˆ‡æ¢åˆ°å‡½æ•°æ¨¡å¼
                    if mode == "message" and tool_call and tool_call.id:
                        logger.debug("ğŸ”§ [DeepSeek] Switching from message to function mode")
                        mode = None
                        await event_stream.send_text_message_end({"messageId": current_message_id})
                    
                    # ä»å‡½æ•°æ¨¡å¼åˆ‡æ¢åˆ°æ¶ˆæ¯æ¨¡å¼æˆ–æ–°å‡½æ•°
                    elif mode == "function" and (not tool_call or tool_call.id):
                        logger.debug("ğŸ”§ [DeepSeek] Switching from function to message/new function mode")
                        mode = None
                        await event_stream.send_action_execution_end({"actionExecutionId": current_tool_call_id})
                    
                    # å¼€å§‹æ–°çš„æ¨¡å¼
                    if mode is None:
                        if tool_call and tool_call.id:
                            logger.debug("ğŸš€ [DeepSeek] Starting function mode")
                            mode = "function"
                            current_tool_call_id = tool_call.id
                            current_action_name = tool_call.function.name if tool_call.function else ""
                            
                            await event_stream.send_action_execution_start({
                                "actionExecutionId": current_tool_call_id,
                                "parentMessageId": chunk.id,
                                "actionName": current_action_name
                            })
                        
                        elif content:
                            logger.debug("ğŸ’¬ [DeepSeek] Starting message mode")
                            mode = "message"
                            current_message_id = chunk.id or generate_id()
                            await event_stream.send_text_message_start({"messageId": current_message_id})
                    
                    # å‘é€å†…å®¹äº‹ä»¶
                    if mode == "message" and content:
                        logger.debug(f"ğŸ’¬ [DeepSeek] Sending text content: {content}")
                        await event_stream.send_text_message_content({
                            "messageId": current_message_id,
                            "content": content
                        })
                    
                    elif mode == "function" and tool_call and tool_call.function and tool_call.function.arguments:
                        logger.debug(f"ğŸ“ [DeepSeek] Sending function arguments: {tool_call.function.arguments}")
                        await event_stream.send_action_execution_args({
                            "actionExecutionId": current_tool_call_id,
                            "args": tool_call.function.arguments
                        })
                    
                    # å¦‚æœæœ‰ç»“æŸåŸå› ï¼Œè·³å‡ºå¾ªç¯
                    if finish_reason:
                        logger.debug(f"ğŸ”š [DeepSeek] Breaking loop due to finish reason: {finish_reason}")
                        break
                
                # å‘é€æœ€ç»ˆç»“æŸäº‹ä»¶
                logger.info(f"ğŸ [DeepSeek] Stream loop ended after {chunk_count} chunks, sending final events")
                
                if mode == "message":
                    logger.debug("ğŸ’¬ [DeepSeek] Ending final text message")
                    await event_stream.send_text_message_end({"messageId": current_message_id})
                elif mode == "function":
                    logger.debug("ğŸ”§ [DeepSeek] Ending final function execution")
                    await event_stream.send_action_execution_end({"actionExecutionId": current_tool_call_id})
                
            except Exception as error:
                logger.error(f"âŒ [DeepSeek] Streaming error: {error}")
                
                # é”™è¯¯æ¸…ç†
                if mode == "message":
                    logger.debug("ğŸ’¬ [DeepSeek] Error cleanup: ending text message")
                    await event_stream.send_text_message_end({"messageId": current_message_id})
                elif mode == "function" and current_tool_call_id:
                    logger.debug("ğŸ”§ [DeepSeek] Error cleanup: ending function execution")
                    await event_stream.send_action_execution_end({"actionExecutionId": current_tool_call_id})
                
                raise error
            
            # å®Œæˆäº‹ä»¶æµ
            logger.info("ğŸ‰ [DeepSeek] Completing event stream")
            event_stream.complete()
        
        # å¯åŠ¨æµå¤„ç†
        event_source.stream(stream_callback)


# ä¾¿æ·å‡½æ•°
def create_deepseek_adapter(
    api_key: str,
    model: str = DEFAULT_MODEL,
    **kwargs
) -> DeepSeekAdapter:
    """
    åˆ›å»º DeepSeek é€‚é…å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        api_key: DeepSeek API å¯†é’¥
        model: æ¨¡å‹åç§°
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
        
    Returns:
        DeepSeek é€‚é…å™¨å®ä¾‹
    """
    return DeepSeekAdapter(
        api_key=api_key,
        model=model,
        **kwargs
    ) 